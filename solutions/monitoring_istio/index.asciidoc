//Category=Communication;Kubernetes;Microservice Platforms;Monitoring;
//Product=Istio;Grafana;
//Maturity level=Initial

// Variables

:toc:

= Istio Microservice Monitoring

== Why do I need Istio for my monitoring?

These two articles should give you an idea why you need monitoring and why you would want to configure it with Istio.

To get an understanding of what Istio is and why you should use it regardless of your intend to implement monitoring we recommend you to read their https://istio.io/latest/about/service-mesh/[introduction] about service mesh.

For most of your monitoring questions you might want to read the https://istio.io/latest/docs/concepts/observability/["Observability"] section from the official Istio https://istio.io/latest/docs/[documentation]. 

== Why should you use Istio?

Istio makes traffic management transparent to the application, moving this functionality out of the application and into the platform layer as a cloud native infrastructure. Istio complements Kubernetes, by enhancing its traffic management, observability and security for cloud native applications. On top Istio has much more functions like: secure, connect, and monitor microservices. We will mainly only highlight the monitoring here. Istio was developed at the time specifically for the Kubernetes cluster environment, among other things. Istio should solve the problem of overcoming the loss of observability and communication and interaction control that occurs with a larger number of microservices. Istio acts as an infrastructure layer between the application service and the network to provide a unified (non-language specific) way to control microservices. Theoretically, istio still has many functions: Automatic load balancing, fine-grained traffic control, access control, visibility but we have only covered a fraction of them in this solution. However, this can be seen as a chance to get more out of the tool when istio is already running in the cluster.

//Abstract
== Core elements of the problem and its solution

//Communication:  
//Now, we want to maintain the communication between the individual microservices uniformly by default.

Our solution is tailored for when Istio is running on the existing cluster and monitoring of the various microservices is attempted.
We are in an environment of Docker, Kubernetes, microservices and Istio. The idea here is that a cluster consists of microservices and runs with Docker over Kubernetes.  In addition, we would like to find a way how we can monitor our cluster with the individual microservices and accordingly make some analyses via Prometheus or Grafana. Now we present a solution following for how we have implemented this scenario. Our final project was a cluster with three different namespaces: Istio, Application and Monitoring. On the Istio namespace the normal Istio system is running and the solution was tested with the default demo https://istio.io/latest/docs/setup/additional-setup/config-profiles/[profile]. The application namespace runs all microservices and the monitoring namespace is where we want to do the monitoring.

We used the following tools to solve the problem:

* Docker
* Kubernetes
* Istio
* Grafana
* Prometheus
* Kiali

== Introduction

Assumed is a Kubernetes cluster with Istio installed and running microservices. We will continuously show how you can monitor your cluster with the microservices using Prometheus and Grafana and how you can split the tasks into different namespaces.

// This image fits best here because we cover details later

image::monitoring-namespaces.png[Namespaces Namespaces, width=100%, height=100%]

== Prerequisites 
* basic Docker runs on your environment https://docs.docker.com/get-docker/[(docker install)]
* Kubernetes running with Istio https://istio.io/latest/docs/setup/getting-started/[(istio install)]
* you will need a gateway that exposes your application to incoming traffic 

=== For the future (optional)
If your existing application doesn't satisfy these prerequisites you can setup an istio ingress-gateway by following the https://istio.io/latest/docs/tasks/traffic-management/ingress/ingress-control/[Istio documentation] and adapt the configuration to your needs.

We offer a basic xref:Files/ingressgateway.yaml[ingressgateway.yaml] for this step but the configuration varies drastically depending on your specific application. Configuring an istio-ingressgateway or any other gateway is most likely mandatory but out of scope for this solution. Therefore we have only covered the bare minimum. 

== Requirements
* adapt the walkthrough of deploying the https://istio.io/latest/docs/setup/getting-started/#bookinfo[sample application] to deploy your own application in the application namespace: <<creating_namespaces>>
* split your cluster in 3 seperate namespaces (shown below)

== Goals
Our goal is to have a cluster with 3 namespaces and the monitoring shall be in its own namespace:

. Istio
. Application
. Monitoring

In detail we want the following:

* a standard Istio namespace
* run standard microservices in the application namespace
* intercept the metrics created by Istio and process them by our monitoring namespace

// This image fits here because as a reader your should have and Idea now what istio and kubernetes is -> Monitoring will be covered later
// TODO add description to arrows

image::monitoring-architecture-simple.png[Namespaces Architecture Simple, width=100%, height=100%]
[.small]#Description: Istio as a tool in focus#

=== What is possible in the future?
Since this solution is tailored towards an existing application you may have gateways (like Kubernetes Virtual Service) configured that expose your application to outside traffic already. With Istio you can define traffic routes and destination rules inside your cluster. Monitoring with Istio will help you to analyze the performance of your cluster regardless of your gateway cofiguration. Just note that configuring an ingress-gateway will enable other benefits that are likely going to influence the monitoring of your application.

=== Why monitoring in its own namespace?
For a detailed overview: read the https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/[explanation] of namespaces.

== Context and Scope
*We would like to walk you through our decision making, why we think that you should use Prometheus and Grafana for your monitoring.* + 
We use https://grafana.com/[Prometheus] to intercept the metrics created by Istio. This data is then passed on to https://grafana.com/[Grafana] to visualize the data e.g. with graphs. On top Prometheus provides a powerful querying language. Grafana and Prometheus are both compatible with most data source types. Additionally we recommend you to add https://istio.io/latest/docs/ops/integrations/kiali/[Kiali] to your architecture. +
*Further we would like to explain why we also recommend to use Istio.* Istio is in charge of connect, secure, control, and observe services, but in our solution we only focus on the "connect" feature. How would you document your architecture? Writing a documentaion can be a very difficult and confusing task, as microservices and tools show up left and right in a diverse fashion. And how is anyone going to understand your documentaion? Istio basically allows you to reduce the complexity of your architecture, because all you have to say is, it is managed by Istio.


// TODO - rework solution strategy so that it visualizes final solution
== Solution Strategy
Prometheus is used to intercept and store metrics. In contrast, Grafana is used to visualize the metrics. Kiali displays the structure and state of our Istio cluster. It should be noted that kiali requires Prometheus to create topology structures, calculate health and more. +
The setup of the namespace *istio-system* is indirectly already done, because Istio is already installed on our system and therefore the namespace is created automatically. The next namespace where we don't have to care much is the *Application* namespace, there we only have to add all our microservices which run in our cluster. +
We use Istio's https://istio.io/latest/docs/setup/additional-setup/sidecar-injection/[Sidecar Injection] to enable Istio's features in our namespaces. In particular we want to enable automatic sidecar injection as described in Istio's documentation. We don't recommend using manual sidecar injection unless you know what you are doing.

image::monitoring-architecture.png[Monitoring Architecture, width=100%, height=100%]

[.small]#Description: Monitoring expanded#

== How to implement our solution

=== Create Namespaces [[creating_namespaces]]

Create your namespaces with `istio-injection=enabled` to ensure automatic sidecar injection is on.

* Application
```Kubernetes
  kubectl label namespace application istio-injection=enabled
```

* Monitoring
```Kubernetes
  kubectl label namespace monitoring istio-injection=enabled
```
*Code explanation:* _Create namespaces application/monitoring and enable Istio on namespace_

=== Expose your jobs and microservices

We are defining targets for each of our microservices and jobs, which are scraped through the Kubernetes API server. Where `job` is to be replaced by the name of your microservice. You can add all of it to your Prometheus configuration like we did for our local testing. See xref:Files/prometheus/configmap.yaml[configmap.yaml] for full example +

```YAML
    - job_name: 'job'
      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names:
          - application

      relabel_configs:
      - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: istio-telemetry;prometheus
```
*Code explanation:* _Defining targets for our job_
// This image adds yaml code to the image before
// TODO add description to arrows

image::monitoring-architecture-implementation.png[Monitoring Implementation, width=100%, height=100%] 
[.small]#Description: Visualization of code implementation inside your architecture#

=== Deploy Prometheus and Grafana

The namespace with the *Monitoring* will be a bit more complex, because we have to adjust the config files of Prometheus and Grafana. We have oriented ourselves as it can be seen in this https://istiobyexample.dev/prometheus/[example] +
 *(1) Grafana Monitoring Namespace* - Part 1

Register Grafana as a Service Account to allow it to autheticate while contacting the api server.
```YAML
  ---
# Source: grafana/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: grafana-6.18.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/version: "8.3.1"
    app.kubernetes.io/managed-by: Helm
  name: grafana
  namespace: monitoring
```
Part 2

```YAML
---
# Source: grafana/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: monitoring
  labels:
    helm.sh/chart: grafana-6.18.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/version: "8.3.1"
    app.kubernetes.io/managed-by: Helm
spec:
  #
  type: ClusterIP
  ports:
    - name: service
      port: 3000
      protocol: TCP
      targetPort: 3000

  selector:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: grafana
---
# Source: grafana/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
``` 
Part 3

```YAML
---

apiVersion: v1
data:
  istio-performance-dashboard.json: | [....]
  pilot-dashboard.json: | [....]

kind: ConfigMap
metadata:
  creationTimestamp: null
  name: istio-grafana-dashboards
  namespace: monitoring

---
``` 

Part 4

```YAML
---

apiVersion: v1
data:
  istio-extension-dashboard.json: | [....]
  istio-mesh-dashboard.json: | [....]
  istio-workload-dashboard.json: [....]
  istio-service-dashboard.json: [....]

kind: ConfigMap
metadata:
  creationTimestamp: null
  name: istio-services-grafana-dashboards
  namespace: monitoring

---
``` 
*Code explanation:* _Change config from Grafana to tell in which namespace(monitoring) it should running in_

See xref:Files/grafana.yaml[Grafana] for full example + 
 
  

*(2) Prometheus Monitoring Namespace* - Part 1
 
```YAML
 ---
# Source: prometheus/templates/server/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    component: "server"
    app: prometheus
    release: prometheus
    chart: prometheus-15.0.1
    heritage: Helm
  name: prometheus
  namespace: monitoring
  annotations:
    {}
---
# Source: prometheus/templates/server/cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    component: "server"
    app: prometheus
    release: prometheus
    chart: prometheus-15.0.1
    heritage: Helm
  name: prometheus
  namespace: monitoring
```
Part 2

```YAML
---
# Source: prometheus/templates/server/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    component: "server"
    app: prometheus
    release: prometheus
    chart: prometheus-15.0.1
    heritage: Helm
  name: prometheus
subjects:
  - kind: ServiceAccount
    name: prometheus
    namespace: monitoring
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
---
# Source: prometheus/templates/server/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    component: "server"
    app: prometheus
    release: prometheus
    chart: prometheus-15.0.1
    heritage: Helm
  name: prometheus
  namespace: monitoring
spec:
  ports:
    - name: http
      port: 9090
      protocol: TCP
      targetPort: 9090
  selector:
    component: "server"
    app: prometheus
    release: prometheus
  sessionAffinity: None
  type: "ClusterIP"
---
# Source: prometheus/templates/server/deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    component: "server"
    app: prometheus
    release: prometheus
    chart: prometheus-15.0.1
    heritage: Helm
  name: prometheus
  namespace: monitoring
``` 

*Code explanation:* _Change config from Prometheus to tell in which namespace(monitoring) it should running in_

See xref:Files/prometheus/deployment.yml[Prometheus] for full example

The tools we used for our local testing were Rancher Desktop, Kubernetes, Istio, Grafana and Prometheus. (instead of Rancher Desktop you can use anything that supports Docker) +
Docker to build our Docker Images for the Kubernetes Cluster https://docs.docker.com/[(more about Docker)]. + 
Rancher Desktop because it ran docker and rancher provides you with a local kubernetes cluster https://docs.rancherdesktop.io/[(more about Rancher Desktop)]. +
Kubernetes to integrate the microservices into our cluster https://kubernetes.io/docs/home/[(more about Kubernetes)]. +
Istio ultimately for all the communication and for generating the metrics that we want to evaluate for monitoring https://istio.io/latest/docs/[(more about Istio)]. +
Grafana and Prometheus to collect and process the metrics collected by istio https://grafana.com/docs/[(more about Grafana)] and https://prometheus.io/docs/introduction/overview/[(more about Prometheus)].


You need to tell Kiali where to listen for Prometheus: The url consists of service.namespace:PORT
```YAML
---
 external_services:
      custom_dashboards:
        enabled: true
      istio:
        root_namespace: istio-system
      prometheus:
        url: "http://prometheus.monitoring:9090/"
```
*Code explanation:* _Change config from Kiali to tell on which port Prometheus is running_


//Concrete Steps to create the solution

// Not finished yet
First of all, you need the prerequisites as described above. Then it makes sense to start and set up Docker. Now you can build the images for your microservices. After that you can add your microservices directly to the cluster.

=== If you also use Rancher desktop pay attention to the following things:
Rancher Desktop using "dockerd(moby)" and not "containerd" under the Kubernetes Setting - Container Runtime. Also note that there may be difficulties trying to start the cluster if you are connected to a VPN. After Rancher Desktop has started the cluster add your microservices as you like. 

**Important is to add them directly into the namespace: Application.** 

Create Namespace(*directly with istio enabled*): 
```KUBERNETES
---
kubectl label namespace application istio-injection=enabled
``` 
*Code explanation:* _Creates namespace(Application) with Istio enabled on namespace_


Add microservice retroactively to our application namespace:
```KUBERNETES
---
 kubectl apply -f MICROSERVICE.yaml -n application 
```
*Code explanation:* _Add microservice to our created namespace(Application)_


Now you can install Istio on your cluster. You only have to install Istio in general as described above. Afterwards you can activate Istio on single namespaces as soon as Istio is installed on the cluster. To enable Istio on our application namespace we are done, because the namespace created as described above already enabled it. 

Now our cluster should already have our microservices running under the application namespace, Istio should be installed and enabled on our namespace and now only the monitoring is missing. For this we focus on Grafana and Prometheus. With the Istio installation Grafana and Prometheus are directly provided (istio\samples\addons). Now it is important not to use the standard config files of the monitoring tools, because they will be installed on the istio namespace and run over it. However we want to run them on our own monitoring namespace. Therefore we have to change the config files (grafana.yaml/prometheus.yaml). To do this you can follow our sample code from above. This shows an example of how to edit the config files to run on the separate monitoring namespace. Once you have customized your config files, you can enable them on your cluster with the simple kubernetes command. 

*Apply Prometheus:*
```KUBERNETES
---
kubectl apply -f prometheus.yaml
```

*Apply Grafana:*
```KUBERNETES
---
kubectl apply -f grafana.yaml
```

*Apply Kiali:*
```KUBERNETES
---
kubectl apply -f kiali.yaml
```
*Code explanation:* _Applies our new config files to the cluster_

*This way we now have our tasks divided into the different namespaces and can still use each service as usual.*


== Conclusion
For our purpose, the solution has worked optimally. However, everyone must see whether the solution is applicable to the individual project. Before you implement the changes to your project, make sure it is what you need for your project. Istio is obviously a main component of this solution. If you think Istio is not right for your project, maybe take a closer look at https://linkerd.io/2.11/overview/[linkerd]. An immediate difference between the two is the proxy technology used in the data plane. While Istio uses Envoy as a proxy, Linkerd uses a special proxy called Linkerd-Proxy. Another important difference is that the Linkerd service mesh is created with a Kubernetes mindset, while Istio is suitable for both Kubernetes and non-Kubernetes environments. Therefore, Linkerd can only be run in Kubernetes environments. 

In this solution you created three namespaces. On every namespace you enabled istio. For your monitoring tools you edit the config for your personal use. In our example we changed the namespace to our monitoring namespace. As well you configured the gateway for access and set up Kiali to work with it. Every code we provided is just sample code and could deviate to your use of code. 

Finally, you should now have a kubernetes cluster with three different namespaces: Application, Istio and Monitoring. Istio is platform-independent. This facilitates collaboration with the Kubernetes engine. By using the two functions together, service-to-service and pod-to-pod communication can be secured at the application and network level. Where the microservices run on Application, the Istio-System on Istio and our monitoring tools on Monitoring. Nevertheless it should be possible to evaluate the metrics of the microservices via our monitoring tools.






